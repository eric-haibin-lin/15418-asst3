% You should title the file with a .tex extension (hw1.tex, for example)
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{float}
\usepackage{tikz}
\usepackage{graphicx}

\oddsidemargin0cm
\topmargin-2cm     %I recommend adding these three lines to increase the
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myhwnum}{2}
\newcommand{\myname}{Haibin Lin, Yiming Wu}
\newcommand{\myandrew}{haibinl, wyiming}


\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}

\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW\myhwnum}}}      % Note the different brackets!
\rhead{\fancyplain{}{\myname\\ \myandrew}}
\chead{\fancyplain{}{15-618 }}

\begin{document}

\medskip                  % Skip a "medium" amount of space
                                % (latex determines what medium is)
                                % Also try: \bigskip, \littleskip

\thispagestyle{plain}
\begin{center}                  % Center the following lines
{\Large 15-418 Assignment \myhwnum} \\
\myname \\
\myandrew \\
29 Jan 2016 \\
\end{center}

\question{Part 1}{SAXPY}

\part{Question 1}

%Writeup. Your write-up should contain a detailed description your optimization process and we will grade the writeup on the quality of this description. Make sure you tell us how you implemented (and parallelized) vertexMap and edgeMap, as well as how you chose to represent vertexSubsets. In addition to your algorithm description, we are looking for you to analyze the performance of your program. Specifically address the following questions:

In our implementation, there're two representations of the same vertex set - sparse and dense. For a sparse vertex set, the vertices in the set are represented in an array with their vertex id's. The valid length of the array is equal to the number of vertices in the set. For a dense vertex set, the vertices in the set are represented by a bitmap - where 0 denotes an absent vertex, 1 denotes a present vertex. The length of the bitmap is always the number of all vertices in the graph. These two representation directly affects whether to take top down approach or bottom up approach in edgeMap. 

Implementation of vertexMap 

Where is the synchronization in your solution? Did you do anything to limit the overhead of synchronization?
Why do you think your code (and the TA's reference code) is unable to achieve perfect speedup? (Is it workload imbalance? communication/synchronization? data movement?)
At high thread counts, do you observe a drop-off in performance? If so, (and you may not) why do you think this might be the case?
If you modified the apps, please describe the modifications and the reason behind your change.

\clearpage

\question{Part 3}{Circle Renderer}

%Aspects of your work that you should mention in the write-up include:

%Replicate the score table generated for your solution and specify which machine you ran your code on.

\textbf{(Score)}

\begin{figure}[h!]
\centering
   \includegraphics[width=0.7\textwidth]{score}
   \caption{score tested on NVIDIA GTX 480 GPU}
\end{figure}

%Describe how you decomposed the problem and how you assigned work to CUDA thread blocks and threads (and maybe even warps).

\textbf{(Approach)}

We decomposed the problem into two phases: preprocessing and rendering.

The idea is to split the image into 16 x 16 boxes, where each box contains 48 x 48 pixels. In preprocessing phase, for each box, we perform the circleInBox test to see which of the circles may intersect with it, and store the indices of these circles for further use. When rendering the pixels contained in this box, instead of examining every circle, the pixel only has to check whether it falls in the circles which pass the circleInBox test previously.

\begin{figure}[H]
\centering
   \includegraphics[width=0.5\textwidth]{process.png}
   \caption{We divide the picture into boxes and try to find out which circles have a part overlapping with the boxes.}
\end{figure}


All of the code is now in our \texttt{countCircles} fucntion. Now we describe the details of these two phases as following:

In the preprocessing phase, we produce the circle indices in two steps.
In step 1, we perform circleInBox test to mark 1 for circles which may intersect with the box, mark 0 otherwise.
We launch $boxNumber$ * $numCircle$ CUDA threads in total(256 threads per block), where each thread examines a circle for a box.
The result is stored in a array of length $boxNumber$ * $numCircle$.
The code is in \texttt{kernel\_findCirclesInBoxes}.

In step 2, we run \texttt{thrust} library exclusive scan to collect the indices of these circles who are marked with 1.
When collecting the indices, we launch $boxNumber$ * numCircle CUDA threads to get the index of the circle (when prefix sum increases).
Below is an example array content of the case where box $\#0$ intersects with circle $\#1$ and circle $\#2$, and box $\#1$ intersects with circle $\#0$ and circle $\#2$.
The code is in \texttt{kernel\_getTotalSize} and \texttt{kernel\_findCircles}.

\begin{figure}[h!]
\centering
   \includegraphics[width=0.7\textwidth]{index1}
   \includegraphics[width=0.7\textwidth]{index2}
   \caption{Result of preprocessing phase }
\end{figure}

In the rendering phase, each thread is responsible to render a pixel. It access the preprocessed indices of circles and test if the pixel falls in the circle. The indices of circles are in the ascending order and the pixel is rendered atomically. The number of launched CUDA threads is equal to the number of pixels in the image. In this phase, the threads in each CUDA block are all mapped to the same box in the image.

\textbf{(Synchronization)}

%Describe where synchronization occurs in your solution.
The synchronization in our solution happens due to the fact that the phases has dependencies. Step 2 in the preprocessing phase cannot proceed until step 1 is done. Rendering phase cannot start until preprocessing phase is done. We call $cudaThreadSynchronize()$ to synchronize them.

\textbf{(Communication)}

We also try to reduce the communication between GPU processor and GPU memory. Since each pixel overlaps with multiple circles, render value is buffered locally and not flushed to the memory until the value is updated with all intersecting circles, instead of reading from and writing the value to memory every time.

% What, if any, steps did you take to reduce communication requirements (e.g., synchronization or main memory bandwidth requirements)?

% Briefly describe how you arrived at your final solution. What other approaches did you try along the way. What was wrong with them?

\textbf{(Other solutions we tried)}

We started with a simple approach, where there's no preprocessing and each CUDA thread renders just 1 pixel of the image by checking $numCircle$ circles sequentially. This is slow when the number of circles is large and each pixel falls only in the range of a small number of circles. It's not necessary to check all circles for each pixel. However, this approach work well when $numCircle$ is small (e.g. only 4 circles)

Then we came up with preprocessing with boxes to reduce the number of circles to check for each pixel. The first implementation of this idea generates the indices of intersecting circles for each box sequentially, i.e. we first generate the indices for box $\#0$, then box $\#1$ and so on. Computing this sequentially is not fast, therefore we came up with the idea to store the indices for all boxes in one big array and compute everything in parallel.

We also tried to use shared memory for each block, where the threads in the same block are mapped to the same box and share the same indices of circles. However, the result shows it actually slows down the rendering phase :(

\clearpage
\end{document}

